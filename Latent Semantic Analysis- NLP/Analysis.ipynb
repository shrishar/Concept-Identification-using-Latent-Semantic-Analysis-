{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "import preprocessor as p  #pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Home = os.getcwd() # Run this only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are rules to find out concepts for Thanksgiving week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetching the list of .txt files\n",
    "path = os.path.join(Home,\"data/ThanksGiving\")\n",
    "list_doc = os.listdir(path)\n",
    "os.chdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "\n",
      "Nov 21 2017.txt\n",
      "Nov 25 2017.txt\n",
      "Nov 23 2017.txt\n",
      "Nov 24 2017.txt\n",
      "Nov 26 2017.txt\n",
      "Nov 22 2017.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading\\n\")\n",
    "doc_complete = []\n",
    "for file in list_doc:\n",
    "    print(file)\n",
    "    content = open(file,'r').read().lower()          # lowering all the content\n",
    "    #Using the tweet-preprocessor model to remove:\n",
    "    #URLs, Hashtags, Mentions, Reserved, words(RT, FAV), Emojis, Smileys\n",
    "    doc_complete.append(p.clean(str(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing stop words, Implementing the tf-idf Vectorizing, n-gram model\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update([\"https\",\"http\",\"https://\",\"thanksgiving\"])\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X =vectorizer.fit_transform(doc_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA:\n",
    "\n",
    "Input: X, a matrix where m is number of documents and n is the number of terms.\n",
    "\n",
    "We are going to decompose X into three matrices U, S and T. After the decomposition we determing k, that is the number of concepts that we are going to find. \n",
    "\n",
    "                                           X = U S V^t\n",
    "\n",
    "U will be m x k matrix. The rows will be documents and the columns will be the concepts.\n",
    "S will be k x k diagoal matrix.The elements will be the amount of variations captured from the concepts.\n",
    "V will be m x k (Transpose) matrix. The rows will be terms and the columns will be the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis\n",
    "\n",
    "lsa = TruncatedSVD(n_components=5,n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "happy\n",
      "family\n",
      "day\n",
      "hope\n",
      "great\n",
      "weekend\n",
      "dinner\n",
      "break\n",
      "holiday\n",
      "time\n",
      " \n",
      "Concept 1:\n",
      "break\n",
      "weekend\n",
      "leftovers\n",
      "hope\n",
      "back\n",
      "great\n",
      "christmas\n",
      "mlbb giveaway\n",
      "via\n",
      "mlbb\n",
      " \n",
      "Concept 2:\n",
      "break\n",
      "week\n",
      "america vote\n",
      "america vote yes\n",
      "bless america vote\n",
      "concern god\n",
      "concern god bless\n",
      "discover rescue\n",
      "discover rescue plan\n",
      "ebook discover\n",
      " \n",
      "Concept 3:\n",
      "harvey weinstein accusers\n",
      "accusers spend\n",
      "accusers spend holiday\n",
      "holiday together\n",
      "spend holiday together\n",
      "weinstein accusers spend\n",
      "climb\n",
      "climb record\n",
      "climb record high\n",
      "online sales climb\n",
      " \n",
      "Concept 4:\n",
      "day\n",
      "leftovers\n",
      "yesterday\n",
      "mlbb giveaway\n",
      "mlbb\n",
      "hope\n",
      "giveaway\n",
      "man charged\n",
      "friday\n",
      "class cookbook\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are applying the same rules to find out concepts for Las Vegas Shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "\n",
      "Sep 28 2017.txt\n",
      "Oct 01 2017.txt\n",
      "Sep 29 2017.txt\n",
      "Sep 30 2017.txt\n",
      "Oct 02 2017.txt\n",
      "Oct 03 2017.txt\n",
      "\n",
      "\n",
      "Concept 0:\n",
      "shooting\n",
      "new\n",
      "video\n",
      "like\n",
      "go\n",
      "see\n",
      "via\n",
      "get\n",
      "going\n",
      "one\n",
      " \n",
      "Concept 1:\n",
      "shooting\n",
      "victims\n",
      "massacre\n",
      "prayers\n",
      "shooter\n",
      "mass shooting\n",
      "tragedy\n",
      "mass\n",
      "gun\n",
      "people\n",
      " \n",
      "Concept 2:\n",
      "robbery\n",
      "nine years\n",
      "prison\n",
      "simpson\n",
      "years prison\n",
      "years\n",
      "nine years prison\n",
      "years robbery\n",
      "botched robbery\n",
      "simpson served\n",
      " \n",
      "Concept 3:\n",
      "edc\n",
      "going\n",
      "tickets\n",
      "go\n",
      "kuchar\n",
      "er shooting police\n",
      "jhonattan\n",
      "time\n",
      "like\n",
      "scott\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# fetching the list of .txt files\n",
    "path = os.path.join(Home,\"data/LasVegasShooting\")\n",
    "list_doc = os.listdir(path)\n",
    "os.chdir(path)\n",
    "\n",
    "print(\"Loading\\n\")\n",
    "doc_complete = []\n",
    "for file in list_doc:\n",
    "    print(file)\n",
    "    content = open(file,'r').read().lower()          # lowering all the content\n",
    "    #Using the tweet-preprocessor model to remove:\n",
    "    #URLs, Hashtags, Mentions, Reserved, words(RT, FAV), Emojis, Smileys\n",
    "    doc_complete.append(p.clean(str(content)))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Removing stop words, Implementing the tf-idf Vectorizing, n-gram model\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update([\"https\",\"http\",\"https://\",\"las\",\"las vegas\",\"vegas\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "X =vectorizer.fit_transform(doc_complete)\n",
    "\n",
    "# Latent Semantic Analysis\n",
    "\n",
    "lsa = TruncatedSVD(n_components=4,n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are applying the same rules to find out concepts from 10 days of breaking news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "\n",
      "@SkyNewBreak.txt\n",
      "@BBCBreaking.txt\n",
      "@CBSTopNews.txt\n",
      "@ABCNewsLive.txt\n",
      "@ReutersLive.txt\n",
      "@AJELive.txt\n",
      "@TWCBreaking.txt\n",
      "@WSJbreakingnews.txt\n",
      "@BreakingNews.txt\n",
      "@cnnbrk.txt\n",
      "\n",
      "\n",
      "Concept 0:\n",
      "army\n",
      "first\n",
      "admit\n",
      "admit army\n",
      "admit army invaded\n",
      "army invaded\n",
      "army invaded kanu\n",
      "army must\n",
      "defence minister\n",
      "defence minister admit\n",
      " \n",
      "Concept 1:\n",
      "news\n",
      "8u\n",
      "trump\n",
      "reports\n",
      "let\n",
      "cnn\n",
      "would\n",
      "house\n",
      "says\n",
      "us\n",
      " \n",
      "Concept 2:\n",
      "wakey\n",
      "wakey ignore\n",
      "wakey wakey\n",
      "wakey wakey ignore\n",
      "aide might\n",
      "aide might story\n",
      "army attacks\n",
      "army attacks villages\n",
      "arrested\n",
      "arrested exception\n",
      " \n",
      "Concept 3:\n",
      "reports\n",
      "tillerson\n",
      "says\n",
      "replaced\n",
      "secretary\n",
      "gaddafi\n",
      "cia\n",
      "briefing\n",
      "cia director\n",
      "cia director pompeo\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# fetching the list of .txt files\n",
    "path = os.path.join(Home,\"data/RecentDays\")\n",
    "list_doc = os.listdir(path)\n",
    "os.chdir(path)\n",
    "\n",
    "print(\"Loading\\n\")\n",
    "doc_complete = []\n",
    "for file in list_doc:\n",
    "    print(file)\n",
    "    content = open(file,'r').read().lower()          # lowering all the content\n",
    "    #Using the tweet-preprocessor model to remove:\n",
    "    #URLs, Hashtags, Mentions, Reserved, words(RT, FAV), Emojis, Smileys\n",
    "    doc_complete.append(p.clean(str(content)))\n",
    "\n",
    "print(\"\\n\")\n",
    "# Removing stop words, Implementing the tf-idf Vectorizing, n-gram model\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update([\"https\",\"http\",\"https://\",\"rt\",\"really\",\"decided\"])\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "X =vectorizer.fit_transform(doc_complete)\n",
    "\n",
    "# Latent Semantic Analysis\n",
    "\n",
    "lsa = TruncatedSVD(n_components=4,n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
